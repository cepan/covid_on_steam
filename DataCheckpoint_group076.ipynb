{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Brandon Vazquez\n",
    "- Ernesto Escusa\n",
    "- Chung En Pan\n",
    "- Manuel Rodriguez Nunez\n",
    "- Eric Estabaya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='research_question'></a>\n",
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How has the COVID-19 infection rate influenced the video game community?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset Name: covid-19-data (ouroworldindata.org) 16.1MB\n",
    "- Link to the dataset: https://github.com/owid/covid-19-data/tree/master/public/data\n",
    "- Number of observations: 62827 rows x 59 columns\n",
    "\n",
    "This dataset is the global covid dataset, containing 192 countries infomation about the total of cases, new cases each day, total death, new death each day, total case per million, new case per million, new death/ total death per million, etc between 2020/1/1 to 2021/1/29.\n",
    "\n",
    "- Dataset Name: Steam Games Dataset : Player count history, Price history and data about games 6.53GB\n",
    "- Link to the dataset: https://data.mendeley.com/datasets/ycy3sy3vj2/1\n",
    "- files Name: PlayercountHistoryPart1, PlayercountHistoryPart2, PriceHistory, Developers, Genres, Information, Packages, Supportedlanguages, Tags\n",
    "- Number of observations:\n",
    "    - PlayercountHistoryPart1: containing 1000 (game id) csvs, each of them 280224 rows x 2 column containing player count in each 5 minute interval\n",
    "    - PlayercountHistoryPart2: containing 1000 (game id) csvs, each of them 23352 rows x 2 column containing player count in each 1 hour interval\n",
    "    - PriceHistory: Containing 1513 (game id) csvs, each of them 493 rows x 4 column containing date, initial price, discounted price, and discouted rate\n",
    "    - Developers: 2000 x 2 (game_id, developers)\n",
    "    - Genres: 2000 x various (game_id, generes)\n",
    "    - Information: 2000 x 5 (game_id, type, name, releasedate, freetoplay)\n",
    "    - Packages: 1797 x 3 (game_id, package_id)\n",
    "    - Supportedlanguages: 1993 x 1 (game_id, languages)\n",
    "    - Tags: 2000 x 21 (game_id, tag1- 20)\n",
    "    \n",
    "    \n",
    " We planned to combine the steam data with its information, price history and the player count history, and support languages to get an understanding of each data. Thus, developers, genres, packages, tags csvs will probably be discarded. Steam data and covid data will be compared and investigated rather then combine.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- import os\n",
    "- import pandas\n",
    "- import blob\n",
    "- import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easier data wrangling and visualization later, both the playercounthistory1 and 2 'date' variable are transformed to datetime object, and the average are groupby by the day. Price history 'date' variable are transormed to datetime object and the price  and the rate average are groupby by the day. After transformation, the tables are saved as csv file and later merge with the information csv for later use. This process not only transforms the regular string object to datetime object, but also reduced the observation by a lot (original has more than 280000 observations per file, now to around 1000 observations per file for 2000 files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_datetime(folder):\n",
    "\n",
    "    empty_file= np.array([])\n",
    "    filepath = '~/Github/covid_game_analysis/data/steaminfo/'+ folder\n",
    "    \n",
    "    for filename in os.listdir(filepath):\n",
    "        filename_path = filepath + '/' + filename\n",
    "        if filename.endswith('.csv'):\n",
    "            if os.stat(filename_path).st_size !=0:\n",
    "                with open(os.path.join(filepath, filename),'r+') as f:\n",
    "                    print(filename)\n",
    "                    temp= pd.read_csv(f,dtype={\"Time\": str, \"PlayerCount\": float})\n",
    "                    temp['Time']= pd.to_datetime(temp['Time'], errors='coerce', format='%Y-%m-%d %H:%M')\n",
    "                    temp = temp.dropna(axis=0)\n",
    "                    result= temp.groupby([temp['Time'].dt.date]).mean()\n",
    "                    result['Playercount']= result['Playercount'].apply(int)\n",
    "                    result.to_csv(path_or_buf= filename_path)\n",
    "                \n",
    "        else:\n",
    "            empty_file= np.append(empty_file,filename)\n",
    "    print('empty_file', empty_file)\n",
    "#wrangle_datetime('PlayerCountHistoryPart1')\n",
    "#wrangle_datetime('PlayerCountHistoryPart2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several files needed to be merged based on game id, so I extracted the game_id from information csv and went through the PlayerCountHistory1/2 to extract the time and playercount based on each game_id and then merge them to the corresponding game_id for 2000 game_id. Eventually what I obtained was a single table with game_id, names of the games, released date, time, playercount, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeinfo():\n",
    "    temp_info = pd.read_csv('data/steaminfo/Information.csv', encoding='unicode_escape')\n",
    "    df = pd.DataFrame()\n",
    "    empty_file= np.array([])\n",
    "    for id in temp_info.get('appid'):\n",
    "        \n",
    "        file_path = 'data/steaminfo/PlayerCountHistory/' + str(id) + '.csv' \n",
    "        if os.stat(file_path).st_size !=0:\n",
    "            with open(file_path, 'r') as f:\n",
    "                f_csv = pd.read_csv(f,dtype={\"Time\": str, \"Playercount\": float})\n",
    "                f_csv['appid'] = id\n",
    "                temp = pd.DataFrame()\n",
    "                temp = temp.append(temp_info[temp_info.get('appid') == id])\n",
    "                temp = temp.merge(f_csv, how = 'inner', on = 'appid')\n",
    "                df = df.append(temp)\n",
    "                \n",
    "        else:\n",
    "            empty_file= np.append(empty_file,id)\n",
    "    print('empty_file:',empty_file)\n",
    "    df['Time']= pd.to_datetime(df['Time'], errors='coerce', format='%Y-%m-%d')\n",
    "    return df\n",
    "#df = mergeinfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of the function will notify me whether the files corresponding to the game_id are existed or has no content  in the file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Proposal (updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 1/21 | 9:00 PM |    | Research on final project topic; find the hypothesis regarding the topics; divide tasks     |\n",
    "| 1/28 | 9:30 PM | Research topic decided; divide tasks for project proposal; begin rough draft for proposal        | Finishing up rough draft project proposal, find the ideal dataset | \n",
    "| 2/5  | 9:00 PM | Search for potential datasets to be used in the project                                          | Discuss data that has been found, researching reliability and credibility |\n",
    "| 2/12 | 9:00 PM | Continue to search for potential datasets, reducing and compacting data where necessary          | Discuss all data that has been found and what approach to take from this point, start data wrangling |\n",
    "| 2/19 | ?????   | Continue wrangling data and begin EDA and data cleaning to format our needs                      | Discuss progress on data cleaning, check-in to ensure there are no issues |\n",
    "| 2/26 | ?????   | Continue data cleaning for EDA checkpoint                                                        | Finalize data and prepare for EDA checkpoint submission, discuss data analysis |\n",
    "| 3/5  | ?????   | Begin data analysis and interpretation of figures, charts, etc.                                  | Discuss progress on data analysis, check-in for any oversights and revisions to the dataset |\n",
    "| 3/12 | ?????   | Continue data analysis and complete a rough draft of the analysis                                | Discuss revisions for the rough draft of the analysis, begin planning for video |\n",
    "| 3/18 | ?????   | Work toward a final draft of the analysis, get a video script and recording                      | Finalize the data analysis and video portion of the project |\n",
    "| 3/19 | Before 11:59 PM  | Fix typos and oversights in the analysis and video | Project submission |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
